{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Detecting Malicious URLs using Machine Learning in Python",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NDweq7kqQXS"
      },
      "source": [
        "# Machine Learning for Security Analysts - Workbook </br> Malicious URL Predictor\n",
        "\n",
        "---\n",
        "\n",
        "**Dataset:** https://github.com/VigneshVicky003/Identifying-Malicious-URLs.git\n",
        "\n",
        "**Goal:** This workbook will walk you through the steps to build, train, test, and evaluate a malicious URL predictor using the Scikit-learn machine learning library\n",
        "\n",
        "**Outline:** \n",
        "* Initial Setup\n",
        "* Tokenization\n",
        "* Load Training Data\n",
        "* Vectorize Training Data\n",
        "* Load Testing Data\n",
        "* Train and Evaluate Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWatBgyBZEZy"
      },
      "source": [
        "# Initial Setup\n",
        "We'll start by downloading the data and loading the needed libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctfYiOVJ10FF"
      },
      "source": [
        "# Download data from Github\n",
        "! git clone https://github.com/VigneshVicky003/Identifying-Malicious-URLs.git\n",
        "  \n",
        "# Install dependencies\n",
        "! pip install nltk sklearn pandas matplotlib seaborn\n",
        "data_dir = \"Machine-Learning-for-Security-Analysts/Malicious URLs.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3llO4xb-eTK"
      },
      "source": [
        "# Common imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import re\n",
        " \n",
        "%matplotlib inline\n",
        " \n",
        "# Import Scikit-learn helper functions\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        " \n",
        "# Import Scikit-learn models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        " \n",
        "# Import Scikit-learn metric functions\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        " \n",
        " \n",
        " \n",
        "print(\"\\n### Libraries Imported ###\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2Kzrx8970Zq"
      },
      "source": [
        "# Load the dataset\n",
        "With this set, we first need to load our CSV data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6Cdtwmmd3w2"
      },
      "source": [
        "# Load the training data\n",
        "print(\"- Loading CSV Data -\")\n",
        "url_df = pd.read_csv(data_dir)\n",
        " \n",
        "test_url = url_df['URLs'][4]\n",
        " \n",
        "print(\"\\n### CSV Data Loaded ###\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiEuaSS_8DuT"
      },
      "source": [
        "# Let's see what our training data looks like\n",
        "print(url_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWVv0Lzl8UXi"
      },
      "source": [
        "# Perform Train/Test split\n",
        "test_percentage = .2\n",
        "\n",
        "train_df, test_df = train_test_split(url_df, test_size=test_percentage, random_state=42)\n",
        "\n",
        "labels = train_df['Class']\n",
        "test_labels = test_df['Class']\n",
        "\n",
        "print(\"\\n### Split Complete ###\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb9tYHkw9JTW"
      },
      "source": [
        "# Print counts of each class\n",
        "print(\"- Counting Splits -\")\n",
        "print(\"Training Samples:\", len(train_df))\n",
        "print(\"Testing Samples:\", len(test_df))\n",
        "\n",
        "# Graph counts of each class, for both training and testing\n",
        "count_train_classes = pd.value_counts(train_df['Class'])\n",
        "count_train_classes.plot(kind='bar', fontsize=16)\n",
        "plt.title(\"Class Count (Training)\", fontsize=20)\n",
        "plt.xticks(rotation='horizontal')\n",
        "plt.xlabel(\"Class\", fontsize=20)\n",
        "plt.ylabel(\"Class Count\", fontsize=20)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "count_test_classes = pd.value_counts(test_df['Class'])\n",
        "count_test_classes.plot(kind='bar', fontsize=16, colormap='ocean')\n",
        "plt.title(\"Class Count (Testing)\", fontsize=20)\n",
        "plt.xticks(rotation='horizontal')\n",
        "plt.xlabel(\"Class\", fontsize=20)\n",
        "plt.ylabel(\"Class Count\", fontsize=20)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXws95jzBpcC"
      },
      "source": [
        "# Tokenization\n",
        "Create our tokenizer by splitting URLs into their domains, subdomains, directories, files, and extensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp30x8Ropo_P"
      },
      "source": [
        "# Define tokenizer\n",
        "#   The purpose of a tokenizer is to separate the features from the raw data\n",
        "\n",
        "\n",
        "def tokenizer(url):\n",
        "  \"\"\"Separates feature words from the raw data\n",
        "  Keyword arguments:\n",
        "    url ---- The full URL\n",
        "    \n",
        "  :Returns -- The tokenized words; returned as a list\n",
        "  \"\"\"\n",
        "  \n",
        "  # Split by slash (/) and dash (-)\n",
        "  tokens = re.split('[/-]', url)\n",
        "  \n",
        "  for i in tokens:\n",
        "    # Include the splits extensions and subdomains\n",
        "    if i.find(\".\") >= 0:\n",
        "      dot_split = i.split('.')\n",
        "      \n",
        "      # Remove .com and www. since they're too common\n",
        "      if \"com\" in dot_split:\n",
        "        dot_split.remove(\"com\")\n",
        "      if \"www\" in dot_split:\n",
        "        dot_split.remove(\"www\")\n",
        "      \n",
        "      tokens += dot_split\n",
        "      \n",
        "  return tokens\n",
        "    \n",
        "print(\"\\n### Tokenizer defined ###\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAJK7U4901sx"
      },
      "source": [
        "## Task 1 - Tokenize a URL\n",
        "1. Print the full URL, **test_url**\n",
        "2. Print the results of **tokenizer(test_url)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-ZpjlF47Coq"
      },
      "source": [
        "# Let's see how our tokenizer changes our URLs\n",
        "\n",
        "print(\"\\n- Full URL -\\n\")\n",
        "print(test_url)\n",
        "\n",
        "# Tokenize test URL\n",
        "print(\"\\n- Tokenized Output -\\n\")\n",
        "tokenized_url = tokenizer(test_url)\n",
        "print(tokenized_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJnB-FFwCKse"
      },
      "source": [
        "# Vectorize the Data\n",
        "Now that the training data has been loaded, we'll train the vectorizers to turn our features into numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FpyYEaK0AXK"
      },
      "source": [
        "## Task 2 - Train the vectorizers\n",
        "1. Create the count vectorizer **cVec** using the **CountVectorizer** function\n",
        "2. Configure *cVec* to use the *tokenizer* function from earlier\n",
        "3. Perform **fit_transform** on *cVec* to train the vectorizer with the *training URLs*\\\n",
        "a. Save the result as **count_X**\n",
        "\n",
        "\n",
        "4. Create the TF-IDF vectorizer **tVec** using the **TfidfVectorizer** function\n",
        "5. Configure *tVec* to use the *tokenizer* function from earlier\n",
        "6. Perform **fit_transform** on *tVec* to train the vectorizer with the *training URLs*\\\n",
        "a. Save the result as **tfidf_X** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAfNGyoMCRu7"
      },
      "source": [
        "# Vectorizer the training inputs -- Takes about 30 seconds to complete\n",
        "#   There are two types of vectors:\n",
        "#     1. Count vectorizer\n",
        "#     2. Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "\n",
        "print(\"- Training Count Vectorizer -\")\n",
        "cVec = CountVectorizer(tokenizer=tokenizer)\n",
        "count_X = cVec.fit_transform(train_df['URLs'])\n",
        "\n",
        "print(\"- Training TF-IDF Vectorizer -\")\n",
        "tVec = TfidfVectorizer(tokenizer=tokenizer)\n",
        "tfidf_X = tVec.fit_transform(train_df['URLs'])\n",
        "\n",
        "\n",
        "print(\"\\n### Vectorizing Complete ###\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuOCCZ0N0ITx"
      },
      "source": [
        "## Task 2a - Count the test URL tokens\n",
        "1. Print the count of each *token* from **test_url**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4LjPdSfEP5G"
      },
      "source": [
        "# Manually perform term count on test_url\n",
        "for i in list(dict.fromkeys(tokenized_url)):\n",
        "  print(\"{} - {}\".format(tokenized_url.count(i), i))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iT2Ymr90Ryr"
      },
      "source": [
        "## Task 2b - View the test URL vectorizers\n",
        "1. Create a new **CountVectorizer** and **TfidfVectorizer** for demonstration\n",
        "2. Train the new vectorizers on **test_url** using **fit_transform**\n",
        "3. Print the results of each *transform*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjguwTHgDLBQ"
      },
      "source": [
        "example_cVec = CountVectorizer(tokenizer=tokenizer)\n",
        "example_X = example_cVec.fit_transform([test_url])\n",
        "\n",
        "print(\"\\n- Count Vectorizer (Test URL) -\\n\")\n",
        "print(example_X)\n",
        "\n",
        "print()\n",
        "print(\"=\" * 50)\n",
        "print()\n",
        "\n",
        "example_tVec = TfidfVectorizer(tokenizer=tokenizer)\n",
        "example_X = example_tVec.fit_transform([test_url])\n",
        "\n",
        "print(\"\\n- TFIDF Vectorizer (Test URL) -\\n\")\n",
        "print(example_X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzC_VgqEE7mh"
      },
      "source": [
        "# Test and Evaluate the Models\n",
        "OK, we have our training data loaded and our testing data loaded. Now it's time to train and evaluate our models.\n",
        "\n",
        "But first, we're going to define a helper function to display our evaluation reports."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oybCUH9tzuJN"
      },
      "source": [
        "## Task 3 - Vectorize the testing data\n",
        "1. Use **cVec** to *transform* **test_df['URLs']**\\\n",
        "a. Save the result as **test_count_X**\n",
        "\n",
        "2. Use **tVec** to *transform* **test_df['URLs']**\\\n",
        "a. Save the result as **test_tfidf_X**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYfQhPItHBjz"
      },
      "source": [
        "# Vectorize the testing inputs\n",
        "#   Use 'transform' instead of 'fit_transform' because we've already trained our vectorizers\n",
        "\n",
        "print(\"- Count Vectorizer -\")\n",
        "test_count_X = cVec.transform(test_df['URLs'])\n",
        "\n",
        "print(\"- TFIDF Vectorizer -\")\n",
        "test_tfidf_X = tVec.transform(test_df['URLs'])\n",
        "\n",
        "\n",
        "print(\"\\n### Vectorizing Complete ###\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32pzJ3naFICU"
      },
      "source": [
        "# Define report generator\n",
        "\n",
        "def generate_report(cmatrix, score, creport):\n",
        "  \"\"\"Generates and displays graphical reports\n",
        "  Keyword arguments:\n",
        "    cmatrix - Confusion matrix generated by the model\n",
        "    score --- Score generated by the model\n",
        "    creport - Classification Report generated by the model\n",
        "    \n",
        "  :Returns -- N/A\n",
        "  \"\"\"\n",
        "  \n",
        "  # Transform cmatrix because Sklearn has pred as columns and actual as rows.\n",
        "  cmatrix = cmatrix.T\n",
        "  \n",
        "  # Generate confusion matrix heatmap\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(cmatrix, \n",
        "              annot=True, \n",
        "              fmt=\"d\", \n",
        "              linewidths=.5, \n",
        "              square = True, \n",
        "              cmap = 'Blues', \n",
        "              annot_kws={\"size\": 16}, \n",
        "              xticklabels=['bad', 'good'],\n",
        "              yticklabels=['bad', 'good'])\n",
        "\n",
        "  plt.xticks(rotation='horizontal', fontsize=16)\n",
        "  plt.yticks(rotation='horizontal', fontsize=16)\n",
        "  plt.xlabel('Actual Label', size=20);\n",
        "  plt.ylabel('Predicted Label', size=20);\n",
        "\n",
        "  title = 'Accuracy Score: {0:.4f}'.format(score)\n",
        "  plt.title(title, size = 20);\n",
        "\n",
        "  # Display classification report and confusion matrix\n",
        "  print(creport)\n",
        "  plt.show()\n",
        "  \n",
        "\n",
        "print(\"\\n### Report Generator Defined ###\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOMP_-ymzbuo"
      },
      "source": [
        "## Task 4a - Train and evaluate the MNB-TFIDF model\n",
        "1. Create **mnb_tfidf** as a **MultinomialNB()** constructor\n",
        "2. Use **fit** to train *mnb_tfidf* on the training data (*tfidf_X*) and training labels (*labels*)\n",
        "3. Evaluate the model with the testing data (*test_tfidf_X*) and testing labels (*test_labels*):\\\n",
        "a. Use the **score** function in *mnb_tfidf* to calculate model accuracy; save the results as **score_mnb_tfidf**\\\n",
        "b. Use the **predict** function in *mnb_tfidf* to generate model predictions; save the results as **predictions_mnb_tfidf**\\\n",
        "c. Generate the confusion matrix with **confusion_matrix**, using the predictons and labels; save the results as **cmatrix_mnb_tfidf**\\\n",
        "d. Generate the classification report with **classification_report**, using the predictions and labels; save the results as **creport_mnb_tfidf**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_8Ii5LnFUMy"
      },
      "source": [
        "# Multinomial Naive Bayesian with TF-IDF\n",
        "\n",
        "# Train the model\n",
        "mnb_tfidf = MultinomialNB()\n",
        "mnb_tfidf.fit(tfidf_X, labels)\n",
        "\n",
        "\n",
        "# Test the mode (score, predictions, confusion matrix, classification report)\n",
        "score_mnb_tfidf = mnb_tfidf.score(test_tfidf_X, test_labels)\n",
        "predictions_mnb_tfidf = mnb_tfidf.predict(test_tfidf_X)\n",
        "cmatrix_mnb_tfidf = confusion_matrix(test_labels, predictions_mnb_tfidf)\n",
        "creport_mnb_tfidf = classification_report(test_labels, predictions_mnb_tfidf)\n",
        "\n",
        "print(\"\\n### Model Built ###\\n\")\n",
        "generate_report(cmatrix_mnb_tfidf, score_mnb_tfidf, creport_mnb_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOYVD36UzYLp"
      },
      "source": [
        "## Task 4b - Train and evaluate the MNB-Count model\n",
        "1. Create **mnb_count** as a **MultinomialNB()** constructor\n",
        "2. Use **fit** to train *mnb_count* on the training data (*count_X*) and training labels (*labels*)\n",
        "3. Evaluate the model with the testing data (*test_count_X*) and testing labels (*test_labels*):\\\n",
        "a. Use the **score** function in *mnb_count* to calculate model accuracy; save the results as **score_mnb_count**\\\n",
        "b. Use the **predict** function in *mnb_count* to generate model predictions; save the results as **predictions_mnb_count**\\\n",
        "c. Generate the confusion matrix with **confusion_matrix**, using the predictons and labels; save the results as **cmatrix_mnb_count**\\\n",
        "d. Generate the classification report with **classification_report**, using the predictions and labels; save the results as **creport_mnb_count**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOl4-P-vHy76"
      },
      "source": [
        "# Multinomial Naive Bayesian with Count Vectorizer\n",
        "\n",
        "# Train the model\n",
        "mnb_count = MultinomialNB()\n",
        "mnb_count.fit(count_X, labels)\n",
        "\n",
        "\n",
        "# Test the mode (score, predictions, confusion matrix, classification report)\n",
        "score_mnb_count = mnb_count.score(test_count_X, test_labels)\n",
        "predictions_mnb_count = mnb_count.predict(test_count_X)\n",
        "cmatrix_mnb_count = confusion_matrix(test_labels, predictions_mnb_count)\n",
        "creport_mnb_count = classification_report(test_labels, predictions_mnb_count)\n",
        "\n",
        "print(\"\\n### Model Built ###\\n\")\n",
        "generate_report(cmatrix_mnb_count, score_mnb_count, creport_mnb_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzpIslDdzVJh"
      },
      "source": [
        "## Task 4c - Train and evaluate the LGS-TFIDF model\n",
        "1. Create **lgs_tfidf** as a **LogisticRegression()** constructor, using the **lbfgs** *solver*\n",
        "2. Use **fit** to train *lgs_tfidf* on the training data (*tfidf_X*) and training labels (*labels*)\n",
        "3. Evaluate the model with the testing data (*test_tfidf_X*) and testing labels (*test_labels*):\\\n",
        "a. Use the **score** function in *lgs_tfidf* to calculate model accuracy; save the results as **score_lgs_tfidf**\\\n",
        "b. Use the **predict** function in *lgs_tfidf* to generate model predictions; save the results as **predictions_lgs_tfidf**\\\n",
        "c. Generate the confusion matrix with **confusion_matrix**, using the predictons and labels; save the results as **cmatrix_lgs_tfidf**\\\n",
        "d. Generate the classification report with **classification_report**, using the predictions and labels; save the results as **creport_lgs_tfidf**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5x6-hlkItbB"
      },
      "source": [
        "# Logistic Regression with TF-IDF\n",
        "\n",
        "# Train the model\n",
        "lgs_tfidf = LogisticRegression(solver='lbfgs')\n",
        "lgs_tfidf.fit(tfidf_X, labels)\n",
        "\n",
        "\n",
        "# Test the mode (score, predictions, confusion matrix, classification report)\n",
        "score_lgs_tfidf = lgs_tfidf.score(test_tfidf_X, test_labels)\n",
        "predictions_lgs_tfidf = lgs_tfidf.predict(test_tfidf_X)\n",
        "cmatrix_lgs_tfidf = confusion_matrix(test_labels, predictions_lgs_tfidf)\n",
        "creport_lgs_tfidf = classification_report(test_labels, predictions_lgs_tfidf)\n",
        "\n",
        "print(\"\\n### Model Built ###\\n\")\n",
        "generate_report(cmatrix_lgs_tfidf, score_lgs_tfidf, creport_lgs_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd2S7nCzzIpX"
      },
      "source": [
        "## Task 4d - Train and evaluate the LGS-Count model\n",
        "1. Create **lgs_count** as a **LogisticRegression()** constructor, using the **lbfgs** *solver*\n",
        "2. Use **fit** to train *lgs_count* on the training data (*count_X*) and training labels (*labels*)\n",
        "3. Evaluate the model with the testing data (*test_count_X*) and testing labels (*test_labels*):\\\n",
        "a. Use the **score** function in *lgs_count* to calculate model accuracy; save the results as **score_lgs_count**\\\n",
        "b. Use the **predict** function in *lgs_count* to generate model predictions; save the results as **predictions_lgs_count**\\\n",
        "c. Generate the confusion matrix with **confusion_matrix**, using the predictons and labels; save the results as **cmatrix_lgs_count**\\\n",
        "d. Generate the classification report with **classification_report**, using the predictions and labels; save the results as **creport_lgs_count**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OTvv0tLItrM"
      },
      "source": [
        "# Logistic Regression with Count Vectorizer\n",
        "\n",
        "# Train the model\n",
        "lgs_count = LogisticRegression(solver='lbfgs')\n",
        "lgs_count.fit(count_X, labels)\n",
        "\n",
        "\n",
        "# Test the mode (score, predictions, confusion matrix, classification report)\n",
        "score_lgs_count = lgs_count.score(test_count_X, test_labels)\n",
        "predictions_lgs_count = lgs_count.predict(test_count_X)\n",
        "cmatrix_lgs_count = confusion_matrix(test_labels, predictions_lgs_count)\n",
        "creport_lgs_count = classification_report(test_labels, predictions_lgs_count)\n",
        "\n",
        "print(\"\\n### Model Built ###\\n\")\n",
        "generate_report(cmatrix_lgs_count, score_lgs_count, creport_lgs_count)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
